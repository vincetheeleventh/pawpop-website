import { NextRequest, NextResponse } from "next/server";
import { fal } from "@fal-ai/client";

// Force Node runtime on Vercel (not Edge)
export const runtime = "nodejs";

// Configure fal client
fal.config({
  credentials: process.env.FAL_KEY || process.env.HF_TOKEN
});

interface FluxDirectRequest {
  imageBuffer?: Buffer;
  imageUrl?: string;
  prompt?: string;
  loraPath?: string;
  loraScale?: number;
  aspectRatio?: string;
  guidanceScale?: number;
  numInferenceSteps?: number;
  seed?: number;
}

export async function POST(req: NextRequest) {
  console.log("üé® /api/flux-direct POST request received");
  
  try {
    const contentType = req.headers.get('content-type');
    let body: FluxDirectRequest;
    let imageBuffer: Buffer | null = null;

    if (contentType?.includes('application/json')) {
      // JSON request with base64 image or URL
      const jsonBody = await req.json();
      body = jsonBody;
      
      if (jsonBody.imageBase64) {
        imageBuffer = Buffer.from(jsonBody.imageBase64, 'base64');
      }
    } else if (contentType?.includes('multipart/form-data')) {
      // Form data with file upload
      const formData = await req.formData();
      const file = formData.get('image') as File;
      
      if (!file) {
        return NextResponse.json({ error: "Image file is required" }, { status: 400 });
      }
      
      imageBuffer = Buffer.from(await file.arrayBuffer());
      body = {
        prompt: formData.get('prompt') as string || 'keep likeness, change pose and style to mona lisa, keep hairstyle',
        loraPath: formData.get('loraPath') as string || 'https://v3.fal.media/files/koala/HV-XcuBOG0z0apXA9dzP7_adapter_model.safetensors',
        loraScale: parseFloat(formData.get('loraScale') as string) || 1.0,
        aspectRatio: formData.get('aspectRatio') as string || '9:16',
        guidanceScale: parseFloat(formData.get('guidanceScale') as string) || 7.5,
        numInferenceSteps: parseInt(formData.get('numInferenceSteps') as string) || 28,
        seed: parseInt(formData.get('seed') as string) || Math.floor(Math.random() * 1000000)
      };
    } else {
      return NextResponse.json({ error: "Unsupported content type" }, { status: 400 });
    }

    // Set defaults
    const {
      prompt = 'keep likeness, change pose and style to mona lisa, keep hairstyle',
      loraPath = 'https://v3.fal.media/files/koala/HV-XcuBOG0z0apXA9dzP7_adapter_model.safetensors',
      loraScale = 1.0,
      aspectRatio = '9:16',
      guidanceScale = 7.5,
      numInferenceSteps = 28,
      seed = Math.floor(Math.random() * 1000000)
    } = body;

    console.log("üìã Request parameters:", {
      prompt,
      loraPath,
      loraScale,
      aspectRatio,
      guidanceScale,
      numInferenceSteps,
      seed,
      hasImageBuffer: !!imageBuffer,
      hasImageUrl: !!body.imageUrl
    });

    if (!imageBuffer && !body.imageUrl) {
      return NextResponse.json({ error: "Either image file or image URL is required" }, { status: 400 });
    }

    let imageUrl = body.imageUrl;

    // Upload image to fal storage if we have a buffer
    if (imageBuffer) {
      console.log("‚òÅÔ∏è Uploading image to fal storage...");
      const imageFile = new File([imageBuffer], 'input-image.jpg', { type: 'image/jpeg' });
      imageUrl = await fal.storage.upload(imageFile);
      console.log("‚úÖ Image uploaded:", imageUrl);
    }

    // Run Flux Kontext LoRA transformation with streaming
    console.log("üé® Running Flux Kontext LoRA transformation...");
    const stream = await fal.stream('fal-ai/flux-kontext-lora', {
      input: {
        image_url: imageUrl!,
        prompt: prompt,
        loras: [{
          path: loraPath,
          scale: loraScale
        }],
        resolution_mode: aspectRatio as "9:16" | "auto" | "match_input" | "1:1" | "16:9" | "21:9" | "3:2" | "2:3" | "4:5" | "5:4" | "3:4" | "4:3" | "9:21"
      }
    });

    console.log("üì° Processing stream...");
    for await (const event of stream) {
      console.log("üìù Stream event:", (event as any).type || 'processing');
    }

    const result = await stream.done();

    console.log("‚úÖ Flux transformation complete!");
    
    if (!result || !result.images || !result.images[0]) {
      throw new Error("No image generated by Flux model");
    }

    const outputImageUrl = result.images[0].url;
    console.log("üñºÔ∏è Generated image URL:", outputImageUrl);

    // Download the result image and return as buffer
    console.log("üì• Downloading transformed image...");
    const imageResponse = await fetch(outputImageUrl);
    if (!imageResponse.ok) {
      throw new Error(`Failed to download image: ${imageResponse.status}`);
    }
    
    const resultBuffer = Buffer.from(await imageResponse.arrayBuffer());
    console.log("‚úÖ Image downloaded, size:", resultBuffer.length, "bytes");

    // Return the transformed image
    return new NextResponse(resultBuffer, {
      status: 200,
      headers: { 
        "Content-Type": "image/png",
        "Cache-Control": "no-store",
        "X-Generated-Image-URL": outputImageUrl,
        "X-Aspect-Ratio": aspectRatio,
        "X-LoRA-Scale": loraScale.toString()
      },
    });

  } catch (error: any) {
    console.error("üí• Flux API Error:", error);
    console.error("üí• Error stack:", error?.stack);
    
    let errorMessage = error?.message || "Unknown error";
    let statusCode = 500;
    
    if (error?.message?.includes('credentials') || error?.message?.includes('unauthorized')) {
      errorMessage = "Invalid API credentials. Please check FAL_KEY or HF_TOKEN environment variable.";
      statusCode = 401;
    } else if (error?.message?.includes('quota') || error?.message?.includes('rate limit')) {
      errorMessage = "API quota exceeded or rate limited. Please try again later.";
      statusCode = 429;
    }
    
    return NextResponse.json({ 
      error: errorMessage,
      details: process.env.NODE_ENV === 'development' ? error?.stack : undefined
    }, { status: statusCode });
  }
}
